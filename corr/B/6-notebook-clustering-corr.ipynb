{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d02c7e10",
   "metadata": {},
   "source": [
    "# Clustering (apprentissage non-supervisé)\n",
    "\n",
    "Pour de nombreux problèmes, il existe des bases de données volumineuses, mais non annotées. Dans ce cas, on souhaite souvent trouver une partition des données qui donne des groupes homogènes. Il existe de nombreux algorithmes de partitionnement de données (*clustering* en anglais) qui utilisent des critères différents d'*homogénéité* et qui, généralement, laisse le choix du nombre de groupes (appelés *clusters*) à l'utilisateur. \n",
    "\n",
    "L'objectif de ce TP est de se familiariser avec les principaux algorithmes de clustering, et des critères qui permettent de sélectionner le nombre de clusters (puisqu'en l'absence d'annotations, on ne peut sélectionner les hyper-paramètres par validation croisée).\n",
    "\n",
    "Crédit : notebook inspiré d'un TP de Nicolas Enjalbert Courrech (INRAE, MIAT Toulouse)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3927f2a9",
   "metadata": {},
   "source": [
    "### Notations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc6f7c9",
   "metadata": {},
   "source": [
    "Soit la matrice de données $X \\in \\mathbb{R}^{n \\times p}$ exprimant $p$ variables pour $n$ individus. Le but est de trouver une partition $P = \\{P_1, \\ldots, P_K\\}$ des $n$ individus où $P_k \\subset P$ est l'ensemble des individus du groupe $k$. On note $\\mu_k \\in \\mathbb{R}^p$ le barycentre des individus contenus dans le groupe $k$ tels que $\\forall k \\in \\{1, ..., K\\}, \\mu_k = \\frac{1}{n_k} \\sum_{i \\in P_k} X_{i,\\cdot}$ avec $n_k = |P_k|$ et $X_{i,\\cdot}$ la $i$-eme ligne de la matrice $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571428c6-7400-469a-b367-d0f8b91f112c",
   "metadata": {},
   "source": [
    "### Librairies utilisées\n",
    "\n",
    "Dans ce TP, on utilisera les librairies `pandas` et `numpy` pour la lecture et le pré-traitement des données, `sklearn` et `scipy` pour l'implémentation et l'interprétation des algorithmes de clustering, et `matplotlib` pour la visualisation des données et des résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2276d53-574b-439a-b0eb-e750ebe5e418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "import sklearn\n",
    "from sklearn import decomposition\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1a03f5",
   "metadata": {},
   "source": [
    "### Préparation du jeu de données \n",
    "\n",
    "Nous allons utiliser les données Iris de Fisher. Notez que les annotations / variables à expliquer (*i.e.*, les espèces d'Iris) vont être utiles pour la visualisation des données et l'interprétation des résultats, mais qu'elles ne seront jamais utilisées par les algorithmes de clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee4f41a-4b8f-4f45-bd60-15e1e4cc8070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credits: https://www.angela1c.com/projects/iris_project/downloading-iris/\n",
    "csv_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "# using the attribute information as the column names\n",
    "col_names = ['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width','Species']\n",
    "iris =  pd.read_csv(csv_url, names = col_names)\n",
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba095191",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = iris[[\"Species\"]]\n",
    "y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60baec4a-2a4a-4370-b30d-e72230ddbddf",
   "metadata": {},
   "source": [
    "On simule un jeu de données non annotées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85292335",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.drop([\"Species\"], axis = 1)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48de81a9-7119-4839-8b65-30fdf97ca48c",
   "metadata": {},
   "source": [
    "On utilise une analyse en composantes principales (ACP) pour réduire la dimension des données, ce qui facilite leur visualisation. On verra comment fonctionne l'ACP dans un autre TP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33d277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components = 2)\n",
    "pca_X = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57db238b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(y_true)\n",
    "str2int = dict((k, v) for (v, k) in enumerate(classes))\n",
    "colors =  np.vectorize(\n",
    "# <CORRECTION>\n",
    "    str2int.get\n",
    "    )(\n",
    "    y_true\n",
    "# </CORRECTION>\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "scatter = plt.scatter(pca_X[:, 0], pca_X[:, 1], c=colors, cmap='plasma')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(handles=scatter.legend_elements()[0], \n",
    "           labels=list(classes))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0fe8fa-d2d4-4035-ac5b-fa3b9a0568c2",
   "metadata": {},
   "source": [
    "## 1. Algorithmes de clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7b885f-f8df-4d76-94f5-9f9991702737",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {\n",
    "    'kmeans': {},\n",
    "    'hier': {},\n",
    "    'dbscan': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0a7123",
   "metadata": {},
   "source": [
    "### 1.1 K-means\n",
    "\n",
    "Utilisez la classe `sklearn.cluster.KMeans` pour catégoriser les données pour $K \\in \\{2, \\ldots, 11\\}$ clusters. Visualisez les clusters obtenus en utilisant la projection sur les deux premières composantes principales. Intuitivement, quel clustering vous semble-t-il le meilleur ? Sur quels critères vous basez vous ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38ada6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <CORRECTION>\n",
    "k_min, k_max = 2, 11\n",
    "n_clustering = k_max - k_min + 1\n",
    "\n",
    "for k in range(k_min, k_max + 1): \n",
    "    kmeans = cluster.KMeans(n_clusters=k).fit(X)\n",
    "    clusters['kmeans']['k=' + str(k)] = kmeans\n",
    "# </CORRECTION>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b013fc-8493-4c86-ae4b-0a661c917770",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 2\n",
    "n_cols = n_clustering // n_rows\n",
    "fig, ax = plt.subplots(n_rows, n_cols, figsize=(15, 10))\n",
    "# <CORRECTION>\n",
    "for i, kmeans_cluster in enumerate(clusters['kmeans'].values()):\n",
    "    ax[i // n_cols, i % n_cols].scatter(pca_X[:, 0], pca_X[:, 1], c=kmeans_cluster.labels_,  cmap='plasma')\n",
    "# </CORRECTION>\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcf183f",
   "metadata": {},
   "source": [
    "### 1.2 Clustering Hierarchique Ascendant\n",
    "\n",
    "Le clustering hiérarchique ascendant est un algorithme de clustering dont le nombre de clusters diminue à chaque itération. A l'initialisation, le nombre de clusters $K$ est égal au nombre de données $n$. Ainsi, toutes les données appartiennent à des clusters différents. A chaque itération, les clusters sont fusionnés selon un critère de similarité. A l'itération $t$, il y a donc $n - t$ clusters. L'algorithme s'arrête lorsque toutes les données appartiennent à un seul cluster.\n",
    "\n",
    "Utilisez la classe `sklearn.cluster.AgglomerativeClustering` pour catégoriser les données de manière hiérarchique avec `n_clusters=None` et `distance_threshold=0`. \n",
    "\n",
    "Les clusters sont centenus dans l'attribut `labels_` : sont-ils informatifs ? Recommencez en augmentant la `distance_threshold`. Comment choisir cette distance ? Le choix de la distance est-il équivalent au choix du nombre de clusters ?\n",
    "\n",
    "Utilisez à nouveau `distance_threshold=0` : que contiennent les attributs `children_` et `distances_` ? Comment évoluent les distances au cours des itérations ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f71c2b-8815-45af-8e6b-4944961067de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <CORRECTION>\n",
    "hier_clustering = cluster.AgglomerativeClustering(n_clusters=None, distance_threshold=0)\n",
    "hier_clustering.fit(X)\n",
    "# </CORRECTION>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9d96f9-bb1f-41fa-8da6-aa550d12971e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <CORRECTION>\n",
    "print(len(np.unique(hier_clustering.labels_)))\n",
    "# </CORRECTION>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee386c1-04a1-41a7-aede-ab75a7ef9bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <CORRECTION>\n",
    "print(f\"shape: {hier_clustering.children_.shape}, min value: {hier_clustering.children_.min()}, max value: {hier_clustering.children_.max()}\")\n",
    "hier_clustering.children_[:10, :]\n",
    "# </CORRECTION>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d880db53-a57c-4be5-b749-4f5def6f1b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "# <CORRECTION>\n",
    "plt.plot(hier_clustering.distances_)\n",
    "plt.xlabel('Itérations')\n",
    "# </CORRECTION>\n",
    "plt.ylabel('Distance entre les clusters fusionnés')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fe6944-ba62-41f3-b866-89f0834bffb6",
   "metadata": {},
   "source": [
    "Un très bon outil de visualisation et d'aide à la décision est le dendogramme. Un dendogramme est un diagramme qui illustre l'arrangement des clusters. Il permet de visusaliser l'ascendance des clusters, et la distance entre les clusters fusionnés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f5705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crédit : https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    \n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "    \n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8669613-9488-4b77-98a6-69b241adf12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <CORRECTION>\n",
    "dist_k3 = 8\n",
    "\n",
    "plot_dendrogram(hier_clustering, truncate_mode=\"level\", p=3)\n",
    "plt.hlines(dist_k3, xmin=0, xmax=160, color='black', linestyle='--')\n",
    "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "plt.ylabel(\"Distance between merged clusters\")\n",
    "# <C/ORRECTION>\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e779559-84bf-4c35-90ac-aa892131b3f2",
   "metadata": {},
   "source": [
    "Tracez sur le dendogramme la `distance_threshold` qui permet d'obtenir 3 clusters. Utilisez le clustering hiérarchique avec cette distance. Calculez le nombre de données par clusters à partir du dendogramme. Vérifiez en Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd354571-32d0-4ff2-a5af-eb73e07349f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <CORRECTION>\n",
    "hier_clustering_k3 = cluster.AgglomerativeClustering(n_clusters=None, distance_threshold=dist_k3)\n",
    "hier_clustering_k3.fit(X)\n",
    "print('n_clusters: ', len(np.unique(hier_clustering_k3.labels_)))\n",
    "# </CORRECTION>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3277e6ba-b017-4335-9aed-314f50c832c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <CORRECTION>\n",
    "fig = plt.figure()\n",
    "plt.scatter(pca_X[:, 0], pca_X[:, 1], c=hier_clustering_k3.labels_, cmap='plasma')\n",
    "plt.show()\n",
    "# </CORRECTION>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351ae9f0-f9f7-4b2d-9213-5ac105ad7772",
   "metadata": {},
   "source": [
    "Testez la méthode de clustering hiérarchique pour 3 clusters et d'autres critères de similarité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5bac6f-6f77-44aa-9297-5064f32eeaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "\n",
    "for linkage in ['ward', 'average', 'complete', 'single']:\n",
    "    cl = cluster.AgglomerativeClustering(n_clusters=k, linkage=linkage).fit(X)\n",
    "    clusters['hier'][linkage] = cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753a2c96-c354-45fd-ade5-a8837c759ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(8,8))\n",
    "\n",
    "# <CORRECTION>\n",
    "ax[0, 0].scatter(pca_X[:, 0], pca_X[:, 1], c=clusters['hier']['ward'].labels_, cmap='plasma')\n",
    "ax[0, 1].scatter(pca_X[:, 0], pca_X[:, 1], c=clusters['hier']['average'].labels_, cmap='plasma')\n",
    "ax[1, 0].scatter(pca_X[:, 0], pca_X[:, 1], c=clusters['hier']['complete'].labels_, cmap='plasma')\n",
    "ax[1, 1].scatter(pca_X[:, 0], pca_X[:, 1], c=clusters['hier']['single'].labels_, cmap='plasma')\n",
    "\n",
    "ax[0,0].set_title(\"Ward\")\n",
    "ax[0,1].set_title(\"Average\")\n",
    "ax[1,0].set_title(\"Complete\")\n",
    "ax[1,1].set_title(\"Single\")\n",
    "# </CORRECTION>\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ad08e6",
   "metadata": {},
   "source": [
    "### 1.3 DBSCAN (Density-Based Spatial Clustering of Applications with Noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088091aa",
   "metadata": {},
   "source": [
    "DBSCAN est un algorithme de clustering basé sur la spacialité qui cherche à trouver des zones à forte densité séparées par des zones à faible densité. La recherche se fait de proche en proche en parcourant une boule de rayon `eps` centrée sur les données. Si aucun voisin n'est inclus dans la boule, alors la zone courante est considérée comme une zone à faible densité, et aucun cluster n'est attribué à la donnée (cluster -1 dans `sklearn.cluster.DBSCAN`).\n",
    "\n",
    "Utilisez `sklearn.cluster.DBSCAN` pour différentes tailles de voisinage. Que se passe-t-il quand la taille est trop petite ? Trop grande ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfba9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <CORRECTION>\n",
    "n_clustering = 10\n",
    "eps = np.linspace(0.1, 2, n_clustering)\n",
    "\n",
    "for i, e in enumerate(eps): \n",
    "    cl_dbscan = cluster.DBSCAN(eps=e).fit(X)\n",
    "    clusters['dbscan']['eps=' + str(e)] = cl_dbscan\n",
    "# </CORRECTION>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d948f36e-0077-4356-b28e-94490f853abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 2\n",
    "n_cols = n_clustering // n_rows\n",
    "\n",
    "norm = Normalize(vmin=-1, vmax=3)\n",
    "\n",
    "fig, ax = plt.subplots(n_rows, n_cols, figsize=(15, 10))\n",
    "for i, dbscan_clusters in enumerate(clusters['dbscan'].values()):\n",
    "    ax[i // n_cols, i % n_cols].scatter(pca_X[:, 0], pca_X[:, 1], c=dbscan_clusters.labels_,  cmap='plasma', norm=norm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523d44a2-4448-446f-b31e-f4c975ca96da",
   "metadata": {},
   "source": [
    "Représentez le nombre de données par cluster avec un histogramme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea6fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <CORRECTION>\n",
    "dbscan_labels = dict((k, cluster.labels_) for k, cluster in clusters['dbscan'].items())\n",
    "pd.DataFrame(dbscan_labels).hist(figsize=(10,10))\n",
    "# </CORRECTION>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3b6f51",
   "metadata": {},
   "source": [
    "## 2. Sélection de modèle "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b0e91d",
   "metadata": {},
   "source": [
    "Comment sélectionner les hyper-paramètres des algorithmes de clustering, qui conditionnent le nombre de clusters ? Prenons l'exemple des K-moyennes. Cela a-t-il du sens de sélectionner le nombre de clusters qui minimise la fonction objective (l'inertie intra-classe) ? Faites un graphe de l'inertie intra-classe (voir l'attribut `inertia_` de `sklearn.cluster.KMeans`) en fonction du nombre de clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3822bc-e8a8-482e-bef4-b0b6f5bc808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <CORRECTION>\n",
    "\n",
    "kmeans_inertia = dict((k, cluster.inertia_) for k, cluster in clusters['kmeans'].items())\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(list(kmeans_inertia.values()))\n",
    "ax.set_xticks(list(range(len(kmeans_inertia.keys()))))\n",
    "ax.set_xticklabels(list(kmeans_inertia.keys()))\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Intra-class inertia')\n",
    "plt.show()\n",
    "\n",
    "# </CORRECTION>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70ceb22",
   "metadata": {},
   "source": [
    "### 2.1 Silhouette"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a599bd20",
   "metadata": {},
   "source": [
    "Le coefficient de la Silhouette mesure l'homogénéité des clusters : sont-ils bien regroupés et éloignés des autres ? Pour les K-moyennes, puis pour chaque méthode de clustering, comparer le coefficient de la silhouette afin de sélectionner le modèle avec le meilleur résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cb61bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <CORRECTION>\n",
    "silhouette_kmeans = {}\n",
    "for params, cluster in clusters['kmeans'].items():\n",
    "    silhouette_kmeans[params] = metrics.silhouette_score(X, cluster.labels_, metric=\"euclidean\")\n",
    "# <C/ORRECTION>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33246d1-29d4-463b-bc50-5d732abf965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.plot(list(silhouette_kmeans.values()))\n",
    "k_list = list(silhouette_kmeans.keys())\n",
    "ax.set_xticks(list(range(len(silhouette_kmeans.keys()))))\n",
    "ax.set_xticklabels(k_list)\n",
    "plt.title(\"Valeur du score silhouette en fonction du nombre de cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c90c300-970d-4ff1-8d94-57e400aa61bb",
   "metadata": {},
   "source": [
    "Le score silouhette pour un clustering est la moyenne du score silouhette pour chaque donnée du clustering. Il peut être utile de visualiser le score pour chaque donnée également."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff53050b-e7a1-42b4-a2ee-5c71ebf07475",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 2\n",
    "cluster_labels = clusters['kmeans'][f'k={n_clusters}'].labels_\n",
    "sample_silhouette_values = metrics.silhouette_samples(X, cluster_labels)\n",
    "\n",
    "y_lower = 10\n",
    "fig, ax1 = plt.subplots(figsize=(8, 6))\n",
    "for i in range(n_clusters):\n",
    "    # Aggregate the silhouette scores for samples belonging to\n",
    "    # cluster i, and sort them\n",
    "    ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "    ith_cluster_silhouette_values.sort()\n",
    "\n",
    "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "\n",
    "    color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "    ax1.fill_betweenx(\n",
    "        np.arange(y_lower, y_upper),\n",
    "        0,\n",
    "        ith_cluster_silhouette_values,\n",
    "        facecolor=color,\n",
    "        edgecolor=color,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "\n",
    "    # Label the silhouette plots with their cluster numbers at the middle\n",
    "    ax1.text(-0.025, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    y_lower = y_upper + 10\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f7061c-ea02-4d2d-81d5-eba4cf620ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette = {}\n",
    "for cluster_name in clusters.keys():\n",
    "    silhouette[cluster_name] = {}\n",
    "    for cluster_param, cluster in clusters[cluster_name].items():\n",
    "        if len(np.unique(cluster.labels_)) > 1:\n",
    "            # <CORRECTION>\n",
    "                silhouette[cluster_name][cluster_param] = metrics.silhouette_score(X, cluster.labels_, metric=\"euclidean\")\n",
    "            # </CORRECTION>\n",
    "        else:\n",
    "            # <CORRECTION>\n",
    "            silhouette[cluster_name][cluster_param] = np.nan\n",
    "            # </CORRECTION>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35a4ac6-fb7a-476c-ad1f-b4630bb7f14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(score_dict):\n",
    "    labels = list(score_dict['kmeans'].keys()) + list(score_dict['hier'].keys()) + list(score_dict['dbscan'].keys())\n",
    "    x = np.arange(len(labels))\n",
    "    values = []\n",
    "    colors = []\n",
    "    for cluster_name in score_dict:\n",
    "        score_values = [score_dict[cluster_name][k] for k in score_dict[cluster_name]]\n",
    "        values.extend(score_values)\n",
    "        \n",
    "        # Assign colors by cluster type\n",
    "        if cluster_name == 'kmeans':\n",
    "            colors.extend(['blue'] * len(score_values))\n",
    "        elif cluster_name == 'hier':\n",
    "            colors.extend(['orange'] * len(score_values))\n",
    "        elif cluster_name == 'dbscan':\n",
    "            colors.extend(['green'] * len(score_values))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15, 6))\n",
    "    bars = ax.bar(x, values, color=colors)\n",
    "    \n",
    "    # Customize x-axis\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend(handles=[\n",
    "        plt.Rectangle((0,0),1,1, color='blue', label='K-means'),\n",
    "        plt.Rectangle((0,0),1,1, color='orange', label='Hierarchical'),\n",
    "        plt.Rectangle((0,0),1,1, color='green', label='DBSCAN')\n",
    "    ], loc='upper right', fontsize=15)\n",
    "    \n",
    "    # Add labels and title\n",
    "    ax.set_ylabel('Score')\n",
    "    plt.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ce4728-6685-4a66-91b0-c42b776d667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_hist(silhouette)\n",
    "ax.set_title('Silhouette Score by Method and Parameters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197fe4cd-3165-425d-9e3e-8bf7d8bc6934",
   "metadata": {},
   "source": [
    "### 2.2 Davies Bouldin Score\n",
    "\n",
    "Le score de Davies Bouldin mesure pour chaque cluster sa similarité avec le cluster le plus proche, mesuré comme le ratio de l'inertie intra-classe sur l'inertie inter-classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2ac09f-8542-4362-82a0-0f48b9d4d5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscore = {}\n",
    "for cluster_name in clusters.keys():\n",
    "    dbscore[cluster_name] = {}\n",
    "    for cluster_param, cluster in clusters[cluster_name].items():\n",
    "        if len(np.unique(cluster.labels_)) > 1:\n",
    "            # <CORRECTION>\n",
    "            dbscore[cluster_name][cluster_param] = metrics.davies_bouldin_score(X, cluster.labels_)\n",
    "            # </CORRECTION>\n",
    "        else:\n",
    "            # <CORRECTION>\n",
    "            dbscore[cluster_name][cluster_param] = np.nan\n",
    "            # </CORRECTION>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b117456-18c2-4f57-8165-6f284b34f941",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_hist(dbscore)\n",
    "ax.set_title('Davies Bouldin Score by Method and Parameters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aba24d7-c075-4072-8b63-20792d481d1c",
   "metadata": {},
   "source": [
    "### 2.3 Table de contingence\n",
    "\n",
    "La table de contingence est un outil statistique simple qui permet de voir rapidement si les individus sont classé dans la même classe entre deux partitions. Deux partitions similaires donneront une matrice diagonale. La table de contingence peut donc être utile pour comparer deux algorithmes de clustering. Néanmoins, sans annotation, elle ne permet pas de sélectionner les hyper-paramètres. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59411207-bf41-42a6-bd1f-7f9380e89e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nb clusters kmeans: \", len(np.unique(clusters['kmeans']['k=2'].labels_)))\n",
    "print(\"Nb clusters hierarchical clustering: \", len(np.unique(clusters['hier']['ward'].labels_)))\n",
    "\n",
    "# <CORRECTION>\n",
    "contingency = metrics.cluster.contingency_matrix(clusters['kmeans']['k=2'].labels_, clusters['hier']['ward'].labels_)\n",
    "# </CORRECTION>\n",
    "contingency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e674dbed",
   "metadata": {},
   "source": [
    "### 2.4 Indice de Rand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2193d6",
   "metadata": {},
   "source": [
    "L'indice de Rand est un score qui permet de mesurer la similitude entre deux partitions. Notons $P_1$ et $P_2$ deux partitions des données désignées par leur indice $i \\in \\{1, \\ldots, n\\}$. Pour calculer l'indice de Rand, on calcule le nombre $a$, $b$, $c$, et $d$ de paires d'éléments $(i,j)$ qui sont : \n",
    "* dans le même groupe dans $P_1$ et le même groupe dans $P_2$,\n",
    "* dans le même groupe dans $P_1$ et dans un groupe différent dans $P_2$,\n",
    "* dans un groupe différent dans $P_1$ et le même groupe dans $P_2$,\n",
    "* dans un groupe différent dans $P_1$ et dans un groupe différent dans $P_2$, respectivement.\n",
    "\n",
    "L'indice de Rand est alors $$ RI = \\frac{a+d}{a+b+c+d}.$$\n",
    "\n",
    "Par rapport à la table de contigence, quel est l'avantage de cette métrique ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9f89ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_score = {}\n",
    "for cluster_name in clusters.keys():\n",
    "    rand_score[cluster_name] = {}\n",
    "    for cluster_param, cluster in clusters[cluster_name].items():\n",
    "        # <CORRECTION>\n",
    "        rand_score[cluster_name][cluster_param] = metrics.rand_score(cluster.labels_, y_true.to_numpy().reshape(-1))\n",
    "        # </CORRECTION>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e48a438-4ec4-4af4-9976-8796db5b1017",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_hist(rand_score)\n",
    "ax.set_title('Rand Score by Method and Parameters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a154277-a656-4e94-ae1b-a178e786496c",
   "metadata": {},
   "source": [
    "Les critères de sélection s'accordent-ils ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffbabf32-cbf4-40a6-9ad0-eacbf1875669",
   "metadata": {},
   "outputs": [],
   "source": [
    "### <CORRECTION> ###\n",
    "import re\n",
    "# transformation de cet énoncé en version étudiante\n",
    "\n",
    "fname = \"6-notebook-clustering-corr.ipynb\" # ce fichier\n",
    "fout  = fname.replace(\"-corr\",\"\")\n",
    "\n",
    "# print(\"Fichier de sortie: \", fout )\n",
    "\n",
    "f = open(fname, \"r\")\n",
    "txt = f.read()\n",
    "\n",
    "f.close()\n",
    "\n",
    "\n",
    "f2 = open(fout, \"w\")\n",
    "f2.write(re.sub(\"<CORRECTION>.*?(</CORRECTION>)\",\" TODO \",\\\n",
    "    txt, flags=re.DOTALL))\n",
    "f2.close()\n",
    "\n",
    "### </CORRECTION> ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
